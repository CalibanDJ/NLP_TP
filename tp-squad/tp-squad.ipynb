{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer\nimport transformers","metadata":{"id":"2b86e84e-f240-421b-8b92-37c56fb2e39d","execution":{"iopub.status.busy":"2021-12-10T14:36:36.465908Z","iopub.execute_input":"2021-12-10T14:36:36.466255Z","iopub.status.idle":"2021-12-10T14:36:36.470526Z","shell.execute_reply.started":"2021-12-10T14:36:36.466216Z","shell.execute_reply":"2021-12-10T14:36:36.469808Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"distilbert-base-uncased\"\nbatch_size = 16","metadata":{"id":"ed2c88c3-4af6-4f4d-ba33-914e4f189803","execution":{"iopub.status.busy":"2021-12-10T14:36:36.472732Z","iopub.execute_input":"2021-12-10T14:36:36.473236Z","iopub.status.idle":"2021-12-10T14:36:36.481704Z","shell.execute_reply.started":"2021-12-10T14:36:36.473189Z","shell.execute_reply":"2021-12-10T14:36:36.480910Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"datasets = load_dataset(\"squad_v2\")\n#datasets = datasets.filter(lambda l, i: i < 22222, with_indices=True)","metadata":{"id":"5699d338-5198-4ef2-b257-c2e14ca5f337","outputId":"8efc682a-f813-4ebb-e87d-e37720f47298","execution":{"iopub.status.busy":"2021-12-10T14:36:36.482907Z","iopub.execute_input":"2021-12-10T14:36:36.483389Z","iopub.status.idle":"2021-12-10T14:36:37.145890Z","shell.execute_reply.started":"2021-12-10T14:36:36.483351Z","shell.execute_reply":"2021-12-10T14:36:37.145263Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6607ee457ce648f6803686b162e9036a"}},"metadata":{}}]},{"cell_type":"code","source":"datasets","metadata":{"id":"d90e21d7-46ef-40d6-91ed-89388103bd96","outputId":"f7c880f0-98a6-438c-dd79-07695b4e6569","execution":{"iopub.status.busy":"2021-12-10T14:36:37.149569Z","iopub.execute_input":"2021-12-10T14:36:37.149965Z","iopub.status.idle":"2021-12-10T14:36:37.159409Z","shell.execute_reply.started":"2021-12-10T14:36:37.149928Z","shell.execute_reply":"2021-12-10T14:36:37.158795Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 130319\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 11873\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"datasets[\"train\"][0]","metadata":{"id":"16152749-86d4-4653-8ecc-c233c8cbf21c","outputId":"a22c6d93-d113-4632-9fa6-9255a490008c","execution":{"iopub.status.busy":"2021-12-10T14:36:37.163753Z","iopub.execute_input":"2021-12-10T14:36:37.166076Z","iopub.status.idle":"2021-12-10T14:36:37.178858Z","shell.execute_reply.started":"2021-12-10T14:36:37.166038Z","shell.execute_reply":"2021-12-10T14:36:37.175211Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'id': '56be85543aeaaa14008c9063',\n 'title': 'Beyoncé',\n 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n 'question': 'When did Beyonce start becoming popular?',\n 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import ClassLabel, Sequence\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML\n\ndef show_random_elements(dataset, num_examples=10):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    for column, typ in dataset.features.items():\n        if isinstance(typ, ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n    display(HTML(df.to_html()))","metadata":{"id":"1644318e-5d6c-4435-98fb-eae665555fdf","execution":{"iopub.status.busy":"2021-12-10T14:36:37.179976Z","iopub.execute_input":"2021-12-10T14:36:37.180387Z","iopub.status.idle":"2021-12-10T14:36:37.192221Z","shell.execute_reply.started":"2021-12-10T14:36:37.180352Z","shell.execute_reply":"2021-12-10T14:36:37.191435Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"show_random_elements(datasets[\"train\"])","metadata":{"id":"40e71dac-366f-4a78-ac2a-b2f6003d01f3","outputId":"3a1b08a8-9c9e-4550-f4bc-aa120848b80d","execution":{"iopub.status.busy":"2021-12-10T14:36:37.193590Z","iopub.execute_input":"2021-12-10T14:36:37.194148Z","iopub.status.idle":"2021-12-10T14:36:37.223351Z","shell.execute_reply.started":"2021-12-10T14:36:37.194110Z","shell.execute_reply":"2021-12-10T14:36:37.222716Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5727ebe03acd2414000deff0</td>\n      <td>Gamal_Abdel_Nasser</td>\n      <td>Nasser remains an iconic figure in the Arab world, particularly for his strides towards social justice and Arab unity, modernization policies, and anti-imperialist efforts. His presidency also encouraged and coincided with an Egyptian cultural boom, and launched large industrial projects, including the Aswan Dam and Helwan City. Nasser's detractors criticize his authoritarianism, his government's human rights violations, his populist relationship with the citizenry, and his failure to establish civil institutions, blaming his legacy for future dictatorial governance in Egypt. Historians describe Nasser as a towering political figure of the Middle East in the 20th century.</td>\n      <td>What century did Nasser rule in?</td>\n      <td>{'text': ['20th'], 'answer_start': [667]}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56dff532231d4119001abf05</td>\n      <td>Pub</td>\n      <td>In Ireland, pubs are known for their atmosphere or \"craic\". In Irish, a pub is referred to as teach tábhairne (\"tavernhouse\") or teach óil (\"drinkinghouse\"). Live music, either sessions of traditional Irish music or varieties of modern popular music, is frequently featured in the pubs of Ireland. Pubs in Northern Ireland are largely identical to their counterparts in the Republic of Ireland except for the lack of spirit grocers. A side effect of \"The Troubles\" was that the lack of a tourist industry meant that a higher proportion of traditional bars have survived the wholesale refitting of Irish pub interiors in the 'English style' in the 1950s and 1960s. New Zealand sports a number of Irish pubs.</td>\n      <td>What country outside Ireland is known for having Irish pubs?</td>\n      <td>{'text': ['New Zealand'], 'answer_start': [664]}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>56d61c4e1c85041400946f1d</td>\n      <td>2008_Sichuan_earthquake</td>\n      <td>The Internet was extensively used for passing information to aid rescue and recovery efforts. For example, the official news agency Xinhua set up an online rescue request center in order to find the blind spots of disaster recovery. After knowing that rescue helicopters had trouble landing into the epicenter area in Wenchuan, a student proposed a landing spot online and it was chosen as the first touchdown place for the helicopters[not in citation given]. Volunteers also set up several websites to help store contact information for victims and evacuees. On May 31, a rescue helicopter carrying earthquake survivors and crew members crashed in fog and turbulence in Wenchuan county. No-one survived.</td>\n      <td>What kind of information were websites set up to store?</td>\n      <td>{'text': ['contact information'], 'answer_start': [514]}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>572abec0be1ee31400cb8201</td>\n      <td>Friedrich_Hayek</td>\n      <td>In Why F A Hayek is a Conservative, British policy analyst Madsen Pirie claims Hayek mistakes the nature of the conservative outlook. Conservatives, he says, are not averse to change – but like Hayek, they are highly averse to change being imposed on the social order by people in authority who think they know how to run things better. They wish to allow the market to function smoothly and give it the freedom to change and develop. It is an outlook, says Pirie, that Hayek and conservatives both share.</td>\n      <td>Pirie believes Hayek to be a conservative for what reason?</td>\n      <td>{'text': ['mistakes the nature of the conservative outlook'], 'answer_start': [85]}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5a36f18395360f001af1b36c</td>\n      <td>Gregorian_calendar</td>\n      <td>To unambiguously specify the date, dual dating or Old Style (O.S.) and New Style (N.S.) are sometimes used with dates. Dual dating uses two consecutive years because of differences in the starting date of the year, or includes both the Julian and Gregorian dates. Old Style and New Style (N.S.) indicate either whether the start of the Julian year has been adjusted to start on 1 January (N.S.) even though documents written at the time use a different start of year (O.S.), or whether a date conforms to the Julian calendar (O.S.) rather than the Gregorian (N.S.).</td>\n      <td>Which system indicates that the date that the Julian date has been adjusted for length?</td>\n      <td>{'text': [], 'answer_start': []}</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5a361ff0788daf001a5f8736</td>\n      <td>Himachal_Pradesh</td>\n      <td>The era of planning in Himachal Pradesh started 1948 along with the rest of India. The first five-year plan allocated ₹ 52.7 million to Himachal. More than 50% of this expenditure was incurred on road construction since it was felt that without proper transport facilities, the process of planning and development could not be carried to the people, who mostly lived an isolated existence in far away areas. Himachal now ranks fourth in respect of per capita income among the states of the Indian Union.</td>\n      <td>What would happen without a proper five-year plan?</td>\n      <td>{'text': [], 'answer_start': []}</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>570602f875f01819005e786f</td>\n      <td>Bird_migration</td>\n      <td>The typical image of migration is of northern landbirds, such as swallows (Hirundinidae) and birds of prey, making long flights to the tropics. However, many Holarctic wildfowl and finch (Fringillidae) species winter in the North Temperate Zone, in regions with milder winters than their summer breeding grounds. For example, the pink-footed goose Anser brachyrhynchus migrates from Iceland to Britain and neighbouring countries, whilst the dark-eyed junco Junco hyemalis migrates from subarctic and arctic climates to the contiguous United States and the American goldfinch from taiga to wintering grounds extending from the American South northwestward to Western Oregon. Migratory routes and wintering grounds are traditional and learned by young during their first migration with their parents. Some ducks, such as the garganey Anas querquedula, move completely or partially into the tropics. The European pied flycatcher Ficedula hypoleuca also follows this migratory trend, breeding in Asia and Europe and wintering in Africa.</td>\n      <td>Where do the dark-eyed junco migrate?</td>\n      <td>{'text': ['arctic climates to the contiguous United States'], 'answer_start': [500]}</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>5ad68d08191832001aa7b196</td>\n      <td>Paper</td>\n      <td>The oldest known archaeological fragments of the immediate precursor to modern paper, date to the 2nd century BC in China. The pulp papermaking process is ascribed to Cai Lun, a 2nd-century AD Han court eunuch. With paper as an effective substitute for silk in many applications, China could export silk in greater quantity, contributing to a Golden Age.</td>\n      <td>What material did paper not replace upon its invention?</td>\n      <td>{'text': [], 'answer_start': []}</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>572a60d8d562191400bc86a5</td>\n      <td>Ottoman_Empire</td>\n      <td>The art of carpet weaving was particularly significant in the Ottoman Empire, carpets having an immense importance both as decorative furnishings, rich in religious and other symbolism, and as a practical consideration, as it was customary to remove one's shoes in living quarters. The weaving of such carpets originated in the nomadic cultures of central Asia (carpets being an easily transportable form of furnishing), and was eventually spread to the settled societies of Anatolia. Turks used carpets, rugs and kilims not just on the floors of a room, but also as a hanging on walls and doorways, where they provided additional insulation. They were also commonly donated to mosques, which often amassed large collections of them.</td>\n      <td>Kilims were used by Turks to provide insulation where?</td>\n      <td>{'text': ['walls and doorways'], 'answer_start': [580]}</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5a297d9803c0e7001a3e17b9</td>\n      <td>United_Nations_Population_Fund</td>\n      <td>UNFPA is the world's largest multilateral source of funding for population and reproductive health programs. The Fund works with governments and non-governmental organizations in over 150 countries with the support of the international community, supporting programs that help women, men and young people:</td>\n      <td>How many countries are not yet in support?</td>\n      <td>{'text': [], 'answer_start': []}</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"96cd1fdc-9129-4a58-997b-d68a5baa433e","outputId":"853cdbfb-89e1-4161-bb89-f53ad9ccd79a","execution":{"iopub.status.busy":"2021-12-10T14:36:37.226985Z","iopub.execute_input":"2021-12-10T14:36:37.228918Z","iopub.status.idle":"2021-12-10T14:36:40.325053Z","shell.execute_reply.started":"2021-12-10T14:36:37.228881Z","shell.execute_reply":"2021-12-10T14:36:40.324187Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.12.5\",\n  \"vocab_size\": 30522\n}\n\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\nloading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.12.5\",\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer(\"What are you doing?\", \"Trying to make give a lesson\")","metadata":{"id":"b5e60416-42b9-4c8a-ab05-ae43bb035fd0","outputId":"79d550a8-65b3-43ad-bdbc-f9498f438a5d","execution":{"iopub.status.busy":"2021-12-10T14:36:40.326618Z","iopub.execute_input":"2021-12-10T14:36:40.326893Z","iopub.status.idle":"2021-12-10T14:36:40.335119Z","shell.execute_reply.started":"2021-12-10T14:36:40.326854Z","shell.execute_reply":"2021-12-10T14:36:40.334127Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 2054, 2024, 2017, 2725, 1029, 102, 2667, 2000, 2191, 2507, 1037, 10800, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"max_length = 512 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.","metadata":{"id":"7bb6cdb3-d455-4811-bfa9-84f4a4025976","execution":{"iopub.status.busy":"2021-12-10T14:36:40.336486Z","iopub.execute_input":"2021-12-10T14:36:40.336940Z","iopub.status.idle":"2021-12-10T14:36:40.343663Z","shell.execute_reply.started":"2021-12-10T14:36:40.336858Z","shell.execute_reply":"2021-12-10T14:36:40.342885Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"len(datasets['train'][:5]['answers'][0][\"text\"][0])","metadata":{"pycharm":{"name":"#%%\n"},"id":"ZpmigvaP_zmS","outputId":"e2c3692d-a525-4b2b-be61-20d971744a2e","execution":{"iopub.status.busy":"2021-12-10T14:36:40.345253Z","iopub.execute_input":"2021-12-10T14:36:40.345599Z","iopub.status.idle":"2021-12-10T14:36:40.355625Z","shell.execute_reply.started":"2021-12-10T14:36:40.345561Z","shell.execute_reply":"2021-12-10T14:36:40.354908Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"17"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    pad_on_right = tokenizer.padding_side == \"right\"\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        stride=doc_stride,\n        truncation=True\n\n    )\n\n    # Here we want to tokenized our data, so give the right data to the tokenizer() defined before\n    # you have an example on how to use it, now you just have to give more parameters see more here : https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils.py\n    # on _encode_plus function\n\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    \n    # Here we want to to add a start and an end index to our tokenized_exampes object \n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        # If you didn't check _encode_plus method, tokenized_examples is a BatchEncoding object\n        # See its method here : https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples['answers'][sample_index] # get the answers from examples for the current sample index\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            # HERE : GET start and end here from answers (you can take a look on \"answers\" thanks to the show_random_elements use at the begining)\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n            \n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            # fill me (hint : use offsets and start/end char)\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples # What do we want to return ?","metadata":{"id":"294e0226-411f-4883-9cba-60fee8c7d02c","execution":{"iopub.status.busy":"2021-12-10T14:36:40.358080Z","iopub.execute_input":"2021-12-10T14:36:40.358333Z","iopub.status.idle":"2021-12-10T14:36:40.374926Z","shell.execute_reply.started":"2021-12-10T14:36:40.358308Z","shell.execute_reply":"2021-12-10T14:36:40.374010Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"features = prepare_train_features(datasets['train'][:5])","metadata":{"id":"de4458a8-3237-43e2-9c08-e3c5f7ab210d","execution":{"iopub.status.busy":"2021-12-10T14:36:40.376574Z","iopub.execute_input":"2021-12-10T14:36:40.377140Z","iopub.status.idle":"2021-12-10T14:36:40.394539Z","shell.execute_reply.started":"2021-12-10T14:36:40.377097Z","shell.execute_reply":"2021-12-10T14:36:40.393812Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)","metadata":{"id":"0c2949d2-0719-4142-bc8c-3a7511609387","outputId":"8e70b76a-b5df-4f37-9fbb-f63584152510","execution":{"iopub.status.busy":"2021-12-10T14:36:40.399887Z","iopub.execute_input":"2021-12-10T14:36:40.400075Z","iopub.status.idle":"2021-12-10T14:36:40.438369Z","shell.execute_reply.started":"2021-12-10T14:36:40.400053Z","shell.execute_reply":"2021-12-10T14:36:40.437650Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Fine tuning","metadata":{"id":"bc5b704e-208f-4962-b7c2-347724296efa"}},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\nfrom transformers import default_data_collator\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"id":"cc86f3c6-6cce-4a48-b52e-92d2f64622c8","outputId":"5428d9b9-172f-475f-c8c8-2cbec0b87c77","execution":{"iopub.status.busy":"2021-12-10T14:36:40.441119Z","iopub.execute_input":"2021-12-10T14:36:40.441303Z","iopub.status.idle":"2021-12-10T14:36:41.896668Z","shell.execute_reply.started":"2021-12-10T14:36:40.441281Z","shell.execute_reply":"2021-12-10T14:36:41.895967Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.12.5\",\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = model_checkpoint.split(\"/\")[-1]\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-squad\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=0.0000001, #Choose a value, this is a model with 66 millions of parameters\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=1,#Choose a value,\n    weight_decay=0.1,#Choose a value, weight decay is a regularization technique applied to the weights of a neural network.,\n    push_to_hub=False,\n    disable_tqdm=False,\n)","metadata":{"id":"9e200e4e-e2cf-42f5-b0ea-dd3f033f3bbf","execution":{"iopub.status.busy":"2021-12-10T14:36:41.897962Z","iopub.execute_input":"2021-12-10T14:36:41.898297Z","iopub.status.idle":"2021-12-10T14:36:41.911258Z","shell.execute_reply.started":"2021-12-10T14:36:41.898256Z","shell.execute_reply":"2021-12-10T14:36:41.910449Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","output_type":"stream"}]},{"cell_type":"code","source":"data_collator = default_data_collator\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"id":"61fef340-ffab-49c9-8012-3e480521bb8a","execution":{"iopub.status.busy":"2021-12-10T14:36:41.912671Z","iopub.execute_input":"2021-12-10T14:36:41.913099Z","iopub.status.idle":"2021-12-10T14:36:41.991731Z","shell.execute_reply.started":"2021-12-10T14:36:41.913060Z","shell.execute_reply":"2021-12-10T14:36:41.990949Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"6ddfca0e-7096-424a-90c4-231f91a95582","outputId":"d36923d4-a950-4e64-cc65-aa059ad303e5","execution":{"iopub.status.busy":"2021-12-10T14:36:41.993121Z","iopub.execute_input":"2021-12-10T14:36:41.993385Z","iopub.status.idle":"2021-12-10T16:08:13.450151Z","shell.execute_reply.started":"2021-12-10T14:36:41.993351Z","shell.execute_reply":"2021-12-10T16:08:13.449362Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 130503\n  Num Epochs = 1\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 8157\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/judupuis/huggingface/runs/7cfttpqm\" target=\"_blank\">distilbert-base-uncased-finetuned-squad</a></strong> to <a href=\"https://wandb.ai/judupuis/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8157' max='8157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8157/8157 1:04:25, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.372700</td>\n      <td>4.059294</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-500\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-500/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-1000\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-1000/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-1000/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-1500\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-1500/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-1500/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-1500/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-2000\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-2000/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-2000/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-2500\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-2500/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-2500/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-2500/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-3000\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-3000/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-3000/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-3000/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-3500\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-3500/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-3500/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-3500/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-4000\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-4000/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-4000/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-4000/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-4000/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-4500\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-4500/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-4500/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-4500/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-4500/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-5000\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-5000/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-5000/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-5000/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-5000/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-5500\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-5500/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-5500/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-5500/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-5500/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-6000\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-6000/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-6000/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-6000/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-6000/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-6500\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-6500/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-6500/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-6500/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-6500/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-7000\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-7000/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-7000/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-7000/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-7000/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-7500\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-7500/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-7500/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-7500/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-7500/special_tokens_map.json\nSaving model checkpoint to distilbert-base-uncased-finetuned-squad/checkpoint-8000\nConfiguration saved in distilbert-base-uncased-finetuned-squad/checkpoint-8000/config.json\nModel weights saved in distilbert-base-uncased-finetuned-squad/checkpoint-8000/pytorch_model.bin\ntokenizer config file saved in distilbert-base-uncased-finetuned-squad/checkpoint-8000/tokenizer_config.json\nSpecial tokens file saved in distilbert-base-uncased-finetuned-squad/checkpoint-8000/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 11969\n  Batch size = 16\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=8157, training_loss=5.043317384428731, metrics={'train_runtime': 5491.4063, 'train_samples_per_second': 23.765, 'train_steps_per_second': 1.485, 'total_flos': 1.7050621513476096e+16, 'train_loss': 5.043317384428731, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"test-squad-trained\")","metadata":{"id":"02a80e5d-32ea-424c-b46c-97126f83507a","execution":{"iopub.status.busy":"2021-12-10T16:08:13.451681Z","iopub.execute_input":"2021-12-10T16:08:13.452453Z","iopub.status.idle":"2021-12-10T16:08:14.131127Z","shell.execute_reply.started":"2021-12-10T16:08:13.452407Z","shell.execute_reply":"2021-12-10T16:08:14.130399Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Saving model checkpoint to test-squad-trained\nConfiguration saved in test-squad-trained/config.json\nModel weights saved in test-squad-trained/pytorch_model.bin\ntokenizer config file saved in test-squad-trained/tokenizer_config.json\nSpecial tokens file saved in test-squad-trained/special_tokens_map.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evalutation","metadata":{"id":"5461e414-69df-422b-9c5c-51fdef16725b"}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nfor batch in trainer.get_eval_dataloader():\n    break\nbatch = {k: v.to(trainer.args.device) for k, v in batch.items()}\nwith torch.no_grad():\n    output = trainer.model(**batch)\noutput.keys()","metadata":{"id":"91da0af9-53cf-400a-9fc1-ef588d2621d0","execution":{"iopub.status.busy":"2021-12-10T16:08:14.132457Z","iopub.execute_input":"2021-12-10T16:08:14.132944Z","iopub.status.idle":"2021-12-10T16:08:14.171877Z","shell.execute_reply.started":"2021-12-10T16:08:14.132907Z","shell.execute_reply":"2021-12-10T16:08:14.171167Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"odict_keys(['loss', 'start_logits', 'end_logits'])"},"metadata":{}}]},{"cell_type":"code","source":"n_best_size = 20\nstart_logits = output.start_logits[0].cpu().numpy()\nend_logits = output.end_logits[0].cpu().numpy()\n# Gather the indices the best start/end logits:\nstart_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\nend_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\nvalid_answers = []\nfor start_index in start_indexes:\n    for end_index in end_indexes:\n        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n            text_ids = batch[\"input_ids\"][0][start_index : (end_index + 1)]\n            text = tokenizer.decode(text_ids)\n            valid_answers.append(\n                {\n                    \"score\": start_logits[start_index] + end_logits[end_index],\n                    \"text\": text # We need to find a way to get back the original substring corresponding to the answer in the context\n                }\n            )","metadata":{"id":"6bf58d01-66a0-4cbb-8ed2-96de4751b335","execution":{"iopub.status.busy":"2021-12-10T16:08:14.173298Z","iopub.execute_input":"2021-12-10T16:08:14.173767Z","iopub.status.idle":"2021-12-10T16:08:14.334071Z","shell.execute_reply.started":"2021-12-10T16:08:14.173731Z","shell.execute_reply":"2021-12-10T16:08:14.333395Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        # Reuse the same than the train feature\n        examples[\"question\"],\n        examples[\"context\"],\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        stride=doc_stride,\n        truncation=True\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i) # same, take the method used for the training data\n        context_index = 1 if pad_on_right else 0\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n            # what do we want to add here ?)\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"id":"5bfa8a39-d528-4090-b6ac-6c98498fe496","execution":{"iopub.status.busy":"2021-12-10T16:08:14.340774Z","iopub.execute_input":"2021-12-10T16:08:14.341466Z","iopub.status.idle":"2021-12-10T16:08:14.352848Z","shell.execute_reply.started":"2021-12-10T16:08:14.341417Z","shell.execute_reply":"2021-12-10T16:08:14.352294Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"pad_on_right = tokenizer.padding_side == \"right\"","metadata":{"id":"b7ce134e-565d-4080-87f8-7676bdbf83ac","execution":{"iopub.status.busy":"2021-12-10T16:08:14.356220Z","iopub.execute_input":"2021-12-10T16:08:14.357027Z","iopub.status.idle":"2021-12-10T16:08:14.374893Z","shell.execute_reply.started":"2021-12-10T16:08:14.356918Z","shell.execute_reply":"2021-12-10T16:08:14.374099Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"validation_features = datasets[\"validation\"].map(\n    prepare_validation_features,\n    batched=True,\n    remove_columns=datasets[\"validation\"].column_names\n)","metadata":{"id":"f6344515-d528-496c-a9fb-55b2df7ba0da","execution":{"iopub.status.busy":"2021-12-10T16:08:14.377498Z","iopub.execute_input":"2021-12-10T16:08:14.377858Z","iopub.status.idle":"2021-12-10T16:08:51.870943Z","shell.execute_reply.started":"2021-12-10T16:08:14.377826Z","shell.execute_reply":"2021-12-10T16:08:51.870187Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"940020ea8fbc45bc9d213a39af0ec8b3"}},"metadata":{}}]},{"cell_type":"code","source":"raw_predictions = trainer.predict(validation_features)","metadata":{"id":"66a887ee-b7c7-4690-b0ba-490d6589fda2","execution":{"iopub.status.busy":"2021-12-10T16:08:51.872403Z","iopub.execute_input":"2021-12-10T16:08:51.872662Z","iopub.status.idle":"2021-12-10T16:10:50.641348Z","shell.execute_reply.started":"2021-12-10T16:08:51.872629Z","shell.execute_reply":"2021-12-10T16:10:50.640655Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n***** Running Prediction *****\n  Num examples = 11969\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='749' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [749/749 01:58]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))","metadata":{"id":"af4c8402-69f6-401a-8e9a-f78177134739","execution":{"iopub.status.busy":"2021-12-10T16:10:50.643932Z","iopub.execute_input":"2021-12-10T16:10:50.644120Z","iopub.status.idle":"2021-12-10T16:10:50.648865Z","shell.execute_reply.started":"2021-12-10T16:10:50.644096Z","shell.execute_reply":"2021-12-10T16:10:50.648049Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"max_answer_length = 30","metadata":{"id":"f4948a75-d417-4794-af41-17569a48860c","execution":{"iopub.status.busy":"2021-12-10T16:10:50.649930Z","iopub.execute_input":"2021-12-10T16:10:50.650312Z","iopub.status.idle":"2021-12-10T16:10:50.661374Z","shell.execute_reply.started":"2021-12-10T16:10:50.650275Z","shell.execute_reply":"2021-12-10T16:10:50.660477Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"start_logits = output.start_logits[0].cpu().numpy()\nend_logits = output.end_logits[0].cpu().numpy()\noffset_mapping = validation_features[0][\"offset_mapping\"]\n# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n# an example index\ncontext = datasets[\"validation\"][0][\"context\"]\n\n# Gather the indices the best start/end logits:\nstart_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\nend_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\nvalid_answers = []\nfor start_index in start_indexes:\n    for end_index in end_indexes:\n        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n        # to part of the input_ids that are not in the context.\n        #print(offset_mapping)\n        if (\n            start_index >= len(offset_mapping)\n            or end_index >= len(offset_mapping)\n            or offset_mapping[start_index] is None\n            or offset_mapping[end_index] is None\n            # Fill me (hint: use your offset_mapping and the start and end indexes)\n        ):\n            continue\n        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n            continue\n        if offset_mapping[start_index]  != None and offset_mapping[end_index] != None \\\n        and start_index <= end_index: # We need to refine that test to check the answer is inside the context\n            start_char = offset_mapping[start_index][0]\n            end_char = offset_mapping[end_index][1]\n            valid_answers.append(\n                {\n                    \"score\": start_logits[start_index] + end_logits[end_index],\n                    \"text\": context[start_char: end_char]\n                }\n            )\n\nvalid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\nvalid_answers","metadata":{"id":"1cc217d0-28da-481d-9191-6d5ae38f87a3","execution":{"iopub.status.busy":"2021-12-10T16:10:50.662762Z","iopub.execute_input":"2021-12-10T16:10:50.663272Z","iopub.status.idle":"2021-12-10T16:10:50.686252Z","shell.execute_reply.started":"2021-12-10T16:10:50.663233Z","shell.execute_reply":"2021-12-10T16:10:50.685355Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"[{'score': 1.9130309,\n  'text': 'Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo'},\n {'score': 1.905153,\n  'text': 'Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries'},\n {'score': 1.9043906, 'text': 'Normandy, a region in France'},\n {'score': 1.8824883,\n  'text': '10th and 11th centuries gave their name to Normandy, a region in France'},\n {'score': 1.8478804,\n  'text': 'Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France'},\n {'score': 1.8473067, 'text': 'Normandy'},\n {'score': 1.8254044,\n  'text': '10th and 11th centuries gave their name to Normandy'},\n {'score': 1.7913506,\n  'text': 'Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France'},\n {'score': 1.7907965,\n  'text': 'Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy'},\n {'score': 1.7860879,\n  'text': 'Norman\" comes from \"Norseman\") raiders and pirates from Denmark'},\n {'score': 1.7715735,\n  'text': 'Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo'},\n {'score': 1.7695056, 'text': 'Norman\" comes from \"Norse'},\n {'score': 1.7376177,\n  'text': 'Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway'},\n {'score': 1.7342668,\n  'text': 'Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy'},\n {'score': 1.7219446,\n  'text': 'Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries'},\n {'score': 1.7172935,\n  'text': 'Normans emerged initially in the first half of the 10th century'},\n {'score': 1.6899021,\n  'text': '11th centuries gave their name to Normandy, a region in France'},\n {'score': 1.6615032,\n  'text': 'Normans (Norman: Nourmands; French: Normands; Latin: Normanni'},\n {'score': 1.6495106,\n  'text': 'Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo'},\n {'score': 1.6446306,\n  'text': 'Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark'}]"},"metadata":{}}]},{"cell_type":"code","source":"datasets[\"validation\"][0][\"answers\"]","metadata":{"id":"d2d8f3e0-4504-4530-b3db-bf7d0c67d21f","execution":{"iopub.status.busy":"2021-12-10T16:10:50.688854Z","iopub.execute_input":"2021-12-10T16:10:50.690406Z","iopub.status.idle":"2021-12-10T16:10:50.698179Z","shell.execute_reply.started":"2021-12-10T16:10:50.690373Z","shell.execute_reply":"2021-12-10T16:10:50.697473Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"{'text': ['France', 'France', 'France', 'France'],\n 'answer_start': [159, 159, 159, 159]}"},"metadata":{}}]},{"cell_type":"code","source":"import collections\n\nexamples = datasets[\"validation\"]\nfeatures = validation_features\n\nexample_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\nfeatures_per_example = collections.defaultdict(list)\nfor i, feature in enumerate(features):\n    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)","metadata":{"id":"98bc2077-c27f-4012-8c68-a0d971edad28","execution":{"iopub.status.busy":"2021-12-10T16:10:50.700240Z","iopub.execute_input":"2021-12-10T16:10:50.700836Z","iopub.status.idle":"2021-12-10T16:11:07.509149Z","shell.execute_reply.started":"2021-12-10T16:10:50.700800Z","shell.execute_reply":"2021-12-10T16:11:07.508392Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            \n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            #start_logits = #get the current start logits \n            #end_logits =  #get the current end logits \n            \n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            # Update minimum null prediction.\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer\n        answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n        predictions[example[\"id\"]] = answer\n\n    return predictions","metadata":{"id":"12c5fc55-5c94-42df-89ed-fe8e31a76abf","execution":{"iopub.status.busy":"2021-12-10T16:11:07.510655Z","iopub.execute_input":"2021-12-10T16:11:07.510910Z","iopub.status.idle":"2021-12-10T16:11:07.527185Z","shell.execute_reply.started":"2021-12-10T16:11:07.510876Z","shell.execute_reply":"2021-12-10T16:11:07.526488Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)","metadata":{"id":"e55d0795-aa6b-4a8d-83b6-6ca58ad2dc4e","execution":{"iopub.status.busy":"2021-12-10T16:11:07.529229Z","iopub.execute_input":"2021-12-10T16:11:07.529676Z","iopub.status.idle":"2021-12-10T16:12:04.874711Z","shell.execute_reply.started":"2021-12-10T16:11:07.529636Z","shell.execute_reply":"2021-12-10T16:12:04.873883Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Post-processing 11873 example predictions split into 11969 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11873 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46c2fd8de7c46c8a146c7b94daef6f8"}},"metadata":{}}]},{"cell_type":"code","source":"metric = load_metric(\"squad_v2\")","metadata":{"id":"63650e41-c081-462f-aa05-f8189fd7948e","execution":{"iopub.status.busy":"2021-12-10T16:12:04.876325Z","iopub.execute_input":"2021-12-10T16:12:04.876845Z","iopub.status.idle":"2021-12-10T16:12:06.148318Z","shell.execute_reply.started":"2021-12-10T16:12:04.876804Z","shell.execute_reply":"2021-12-10T16:12:06.147532Z"},"trusted":true},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac315a66dc24bc7b947ee54eb162427"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/3.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b4c88389476412fa83c51ad0cb02116"}},"metadata":{}}]},{"cell_type":"code","source":"formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]","metadata":{"id":"439355b5-4ff4-4413-abe6-4dad4fb3081b","execution":{"iopub.status.busy":"2021-12-10T16:12:06.151717Z","iopub.execute_input":"2021-12-10T16:12:06.152192Z","iopub.status.idle":"2021-12-10T16:12:06.167811Z","shell.execute_reply.started":"2021-12-10T16:12:06.152157Z","shell.execute_reply":"2021-12-10T16:12:06.167006Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\nmetric.compute(predictions=formatted_predictions, references=references)","metadata":{"id":"11490abc-2a1f-4f08-a1b2-a8801aebfa20","execution":{"iopub.status.busy":"2021-12-10T16:12:06.172110Z","iopub.execute_input":"2021-12-10T16:12:06.172394Z","iopub.status.idle":"2021-12-10T16:12:09.502150Z","shell.execute_reply.started":"2021-12-10T16:12:06.172363Z","shell.execute_reply":"2021-12-10T16:12:09.501503Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'exact': 50.07159100480081,\n 'f1': 50.07159100480081,\n 'total': 11873,\n 'HasAns_exact': 0.0,\n 'HasAns_f1': 0.0,\n 'HasAns_total': 5928,\n 'NoAns_exact': 100.0,\n 'NoAns_f1': 100.0,\n 'NoAns_total': 5945,\n 'best_exact': 50.07159100480081,\n 'best_exact_thresh': 0.0,\n 'best_f1': 50.07159100480081,\n 'best_f1_thresh': 0.0}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"71592959-5ba6-4b80-a5c0-e500a5695a1a"},"execution_count":null,"outputs":[]}]}